{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4deafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28712fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bb7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303fa0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chatbot.txt','r') as file:\n",
    "    txt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdbf9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fe366b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd8cfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc73cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning PERSON\n",
      "Machine Learning PERSON\n",
      "Machine Learning PERSON\n",
      "ML ORG\n",
      "today DATE\n",
      "Recent Articles PERSON\n",
      "Machine Learning\n",
      " \n",
      "Introduction\n",
      "Data ORG\n",
      "Deployment\n",
      "ML ORG\n",
      "Applications\n",
      "Miscellaneous\n",
      " \n",
      "\n",
      "Features of Machine WORK_OF_ART\n",
      "daily DATE\n",
      "Machine Learning ORG\n",
      "Machine Learning PERSON\n",
      "Machine Learning ORG\n",
      "Artificial Intelligence\n",
      "Agents ORG\n",
      "10 CARDINAL\n",
      "Understanding Data Processing\n",
      "Python WORK_OF_ART\n",
      "Create Test DataSets ORG\n",
      "Sklearn\n",
      "Python PERSON\n",
      "Generate PERSON\n",
      "Data Preprocessing in Python ORG\n",
      "2 CARDINAL\n",
      "Label Encoding PERSON\n",
      "Handling Imbalanced Data WORK_OF_ART\n",
      "SMOTE ORG\n",
      "Dummy PERSON\n",
      "Regression Models FAC\n",
      "Classification ORG\n",
      "Types of Learning ORG\n",
      "Supervised Learning\n",
      "Multiclass WORK_OF_ART\n",
      "Gradient Descent :\n",
      "Gradient Descent WORK_OF_ART\n",
      "Stochastic Gradient Descent ORG\n",
      "Mini-Batch Gradient Descent PERSON\n",
      "Python\n",
      "Optimization ORG\n",
      "Gradient Descent\n",
      "Introduction ORG\n",
      "Gradient Optimizer\n",
      "Linear Regression PERSON\n",
      "Linear Regression ORG\n",
      "Normal Equation WORK_OF_ART\n",
      "Simple Linear-Regression ORG\n",
      "Univariate Linear Regression in Python\n",
      "Multiple Linear Regression using Python\n",
      "Multiple Linear Regression WORK_OF_ART\n",
      "Linear Regression ORG\n",
      "Linear Regression Using Tensorflow ORG\n",
      "Practical ORG\n",
      "Simple Linear Regression ORG\n",
      "Linear Regression using ORG\n",
      "Linear ORG\n",
      "Apache NORP\n",
      "MLlib ORG\n",
      "ML ORG\n",
      "Boston Housing Kaggle Challenge ORG\n",
      "Linear Regression ORG\n",
      "Implementation of Polynomial Regression\n",
      "Softmax Regression ORG\n",
      "Understanding Logistic Regression\n",
      "Why Logistic Regression in Classification WORK_OF_ART\n",
      "Logistic Regression ORG\n",
      "Logistic Regression\n",
      "Logistic Regression LOC\n",
      "SVM Hyperparameter Tuning WORK_OF_ART\n",
      "SVM ORG\n",
      "Decision Tree Introduction ORG\n",
      "Software Engineering ORG\n",
      "Random Forest ORG\n",
      "Random Forest Regression ORG\n",
      "Voting Classifier PERSON\n",
      "Sklearn\n",
      "Bagging PERSON\n",
      "ML ORG\n",
      "Types of Learning ORG\n",
      "Unsupervised Learning\n",
      "Supervised WORK_OF_ART\n",
      "Machine Learning FAC\n",
      "KMeans NORP\n",
      "ML ORG\n",
      "K-means++ Algorithm\n",
      "Analysis ORG\n",
      "K-Means Clustering ORG\n",
      "Mini Batch K- PERSON\n",
      "Sklearn\n",
      "Fuzzy Clustering PERSON\n",
      "Spectral Clustering PERSON\n",
      "OPTICS Clustering ORG\n",
      "OPTICS Clustering Implementing ORG\n",
      "Sklearn\n",
      "Hierarchical PERSON\n",
      "Agglomerative NORP\n",
      "Implementing Agglomerative Clustering WORK_OF_ART\n",
      "Sklearn\n",
      "Gaussian Mixture Model\n",
      "Reinforcement Learning PERSON\n",
      "Reinforcement Learning Algorithm : FAC\n",
      "Thompson Sampling\n",
      "Genetic Algorithm for Reinforcement Learning ORG\n",
      "Q-Learning ORG\n",
      "Principal Component Analysis(PCA ORG\n",
      "Principal Component Analysis ORG\n",
      "Mathematical Explanation ORG\n",
      "Linear Discriminant Analysis ORG\n",
      "Generalized Discriminant Analysis ORG\n",
      "GDA ORG\n",
      "Independent Component Analysis ORG\n",
      "Mathematical Explanation WORK_OF_ART\n",
      "ML ORG\n",
      "Stochastic Neighbor Embedding WORK_OF_ART\n",
      "Parameters for Feature Selection\n",
      "Underfitting ORG\n",
      "Overfitting GPE\n",
      "Machine Learning GPE\n",
      "Set 2 PERSON\n",
      "NLTK ORG\n",
      "NLTK ORG\n",
      "NLTK ORG\n",
      "Lemmatization with NLTK\n",
      "Lemmatization WORK_OF_ART\n",
      "NLTK WordNet ORG\n",
      "Python GPE\n",
      "Neural Networks ORG\n",
      "Set 1 PRODUCT\n",
      "Artificial Neural Network ORG\n",
      "Set 2 PERSON\n",
      "Artificial Neural Networks ORG\n",
      "ANN ORG\n",
      "Set 4 ORG\n",
      "Network Architectures ORG\n",
      "Activation PERSON\n",
      "Implementing Artificial Neural Network ORG\n",
      "Convolutional Neural Networks\n",
      "Introduction to Convolution Neural Network ORG\n",
      "Applying Convolutional Neural Network ORG\n",
      "Recurrent Neural Networks\n",
      "Introduction to Recurrent Neural Network ORG\n",
      "Recurrent Neural Networks Explanation ORG\n",
      "Networks(GAN PERSON\n",
      "Text Generation using Gated Recurrent Unit Networks ORG\n",
      "Generative Adversarial Network\n",
      "Introduction to Generative Adversarial Network WORK_OF_ART\n",
      "Generative Adversarial Networks ORG\n",
      "Use Cases of Generative Adversarial Networks\n",
      "Building a Generative Adversarial Network ORG\n",
      "Keras ORG\n",
      "Tensorflow ORG\n",
      "Heroku GPE\n",
      "Deploy a Machine Learning Model ORG\n",
      "Streamlit Library PERSON\n",
      "Machine Learning ORG\n",
      "Gradio NORP\n",
      "API ORG\n",
      "ScrapingHub ORG\n",
      "Rainfall WORK_OF_ART\n",
      "Linear ORG\n",
      "Logistic Regression in PyTorch\n",
      "Kaggle Breast Cancer Wisconsin Diagnosis using Logistic Regression\n",
      "Python WORK_OF_ART\n",
      "Implementation of Movie Recommender System ORG\n",
      "Support Vector Machine ORG\n",
      "12 CARDINAL\n",
      "Coin Puzzle ORG\n",
      "Credit Card Fraud Detection ORG\n",
      "NLP ORG\n",
      "Restaurant ORG\n",
      "NLP Problems ORG\n",
      "the Avengers EndGames Characters\n",
      "How Does Google Use Machine Learning ORG\n",
      "5 CARDINAL\n",
      "Mind-Blowing Ways Facebook Uses Machine Learning ORG\n",
      "Misc PERSON\n",
      "Pattern Recognition ORG\n",
      "Differential Privacy ORG\n",
      "Deep Learning ORG\n",
      "Machine Learning ORG\n",
      "Deep Learning\n",
      "Introduction FAC\n",
      "Deep Learning WORK_OF_ART\n",
      "10 CARDINAL\n",
      "Azure Virtual Machine ORG\n",
      "Machine Learning ORG\n",
      "30 minutes TIME\n",
      "Machine Learning GPE\n",
      "Confusion Matrix in Machine Learning\n",
      "Prerequisites WORK_OF_ART\n",
      "Knowledge of Linear WORK_OF_ART\n",
      "Linear Algebra, Probability, Calculus ORG\n",
      "Machine Learning Tutorial ORG\n",
      "Deep GPE\n",
      "Q.2 PERSON\n",
      "Algorithms PERSON\n",
      "Q.4 PERSON\n",
      "Artificial Intelligence and Machine ORG\n",
      "Construct ORG\n",
      "AI ORG\n",
      "ML ORG\n",
      "AI ORG\n",
      "ML ORG\n",
      "1 CARDINAL\n",
      "Java Tutorial PERSON\n",
      "2 CARDINAL\n",
      "SQL Tutorial PERSON\n",
      "3 CARDINAL\n",
      "Java Collection Tutorial ORG\n",
      "4 CARDINAL\n",
      "5 DATE\n",
      "Automata Tutorial PERSON\n",
      "6 CARDINAL\n",
      "Software Engineering Tutorial ORG\n",
      "7 DATE\n",
      "Mongoose Tutorial ORG\n",
      "8 CARDINAL\n",
      "Learn Data Structures ORG\n",
      "Algorithms PERSON\n",
      "9 DATE\n",
      "JavaScript Tutorial PERSON\n",
      "10 CARDINAL\n",
      "Learn FAC\n",
      "924k+ DATE\n",
      "Geeks PERSON\n",
      "Data Structures ORG\n",
      "Algorithms - Self Paced PERSON\n",
      "118k+ CARDINAL\n",
      "Geeks PERSON\n",
      "Complete Machine Learning & Data Science Program ORG\n",
      "144k+ DATE\n",
      "Geeks PERSON\n",
      "Python Programming Foundation ORG\n",
      "A-143 ORG\n",
      "9th Floor ORG\n",
      "Noida GPE\n",
      "Uttar Pradesh - 201305 ORG\n",
      "GFG App ORG\n",
      "GFG App WORK_OF_ART\n",
      "App Store\n",
      " PERSON\n",
      "Us GPE\n",
      "Media GPE\n",
      "POTD ORG\n",
      "Android App Development PERSON\n",
      "JavaScript\n",
      "Languages\n",
      "Python ORG\n",
      "Java PERSON\n",
      "C++\n",
      "PHP\n",
      " PERSON\n",
      "GoLang GPE\n",
      "SQL ORG\n",
      "R Language\n",
      " PRODUCT\n",
      "Android Tutorial PERSON\n",
      "Array\n",
      "String\n",
      " PERSON\n",
      "Linked List\n",
      "Stack\n",
      "Queue\n",
      "Tree\n",
      "Graph\n",
      "Algorithms\n",
      "Sorting\n",
      "Searching\n",
      "Greedy\n",
      "Dynamic Programming\n",
      "Pattern Searching\n",
      "Recursion\n",
      "Backtracking\n",
      "Web Development\n",
      "HTML\n",
      "CSS WORK_OF_ART\n",
      "JavaScript\n",
      "Bootstrap PRODUCT\n",
      "Database Management System\n",
      "Software Engineering ORG\n",
      "Digital Logic Design\n",
      "Engineering Maths\n",
      "Python\n",
      "Python Programming Examples\n",
      "Django Tutorial\n",
      "Python Projects\n",
      "Python Tkinter\n",
      "OpenCV Python Tutorial\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Conversation ORG\n",
      "1 CARDINAL\n",
      "Gmail PERSON\n",
      "8 CARDINAL\n",
      "8,876 CARDINAL\n",
      "5 CARDINAL\n",
      "Inbox GPE\n",
      "Remote ORG\n",
      "2 days ago DATE\n",
      "5 CARDINAL\n",
      "Job NORP\n",
      "five CARDINAL\n",
      "JUL ORG\n",
      "17 CARDINAL\n",
      "GitHub Link PERSON\n",
      "1 CARDINAL\n",
      "YouTube ORG\n",
      "RTMP ORG\n",
      "YouTube ORG\n",
      "WebRTC LOC\n",
      "WebSocket ORG\n",
      "YouTube ORG\n",
      "2 CARDINAL\n",
      "Docs Clone PERSON\n",
      "Google Docs ORG\n",
      "Google Docs PERSON\n",
      "Google Docs ORG\n",
      "WebSockets ORG\n",
      "3 CARDINAL\n",
      "Meet Clone PERSON\n",
      "Hangouts PERSON\n",
      "Google Meet ORG\n",
      "Google Meet ORG\n",
      "WebRTC LOC\n",
      "4 CARDINAL\n",
      "Google Drive Clone FAC\n",
      "Google Drive FAC\n",
      "PDF ORG\n",
      "Android ORG\n",
      "India GPE\n",
      "Google Drive ORG\n",
      "Google Drive FAC\n",
      "AWS S3 ORG\n",
      "5 CARDINAL\n",
      "New Technical Interview Tool ORG\n",
      "Career Development Office ORG\n",
      "MIT ORG\n",
      "Sloan School of Management ORG\n",
      "LeetCode GPE\n",
      "Docker PERSON\n",
      "five CARDINAL\n",
      "one CARDINAL\n",
      "one CARDINAL\n",
      "two-hour TIME\n",
      "Chat-GPT ORG\n",
      "Remote ORG\n",
      "today DATE\n",
      "Remote ORG\n",
      "2023 DATE\n",
      "548 CARDINAL\n",
      "San Francisco GPE\n",
      "CA 94104 FAC\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ef57b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5cb660e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_context',\n",
       " '_get_array_attrs',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'copy',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_dict',\n",
       " 'from_disk',\n",
       " 'from_docs',\n",
       " 'from_json',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_annotation',\n",
       " 'has_extension',\n",
       " 'has_unknown_spaces',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_ents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'spans',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_dict',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'to_utf8_array',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b49d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''' Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
    "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
    "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f532ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise1 = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de718255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ravi\n",
      "Raju\n",
      "Paris\n",
      "London\n",
      "Dubai\n",
      "Rome\n",
      "Mohan\n",
      "Hyderabad\n"
     ]
    }
   ],
   "source": [
    "noun=[]\n",
    "\n",
    "for chunk in exercise1:\n",
    "#     print(chunk.pos_)\n",
    "    if chunk.pos_ == 'PROPN':\n",
    "        print(chunk)\n",
    "        noun.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e8ec3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1421d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = ''' The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10e8ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise2 = nlp(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de4165f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | SPACE\n",
      "The | DET\n",
      "Top | ADJ\n",
      "5 | NUM\n",
      "companies | NOUN\n",
      "in | ADP\n",
      "USA | PROPN\n",
      "are | AUX\n",
      "Tesla | PROPN\n",
      "Walmart | PROPN\n",
      "Amazon | PROPN\n",
      "Microsoft | PROPN\n",
      "Google | PROPN\n",
      "and | CCONJ\n",
      "the | DET\n",
      "top | ADJ\n",
      "5 | NUM\n",
      "companies | NOUN\n",
      "in | ADP\n",
      "\n",
      " | SPACE\n",
      "India | PROPN\n",
      "are | AUX\n",
      "Infosys | PROPN\n",
      "Reliance | PROPN\n",
      "HDFC | PROPN\n",
      "Bank | PROPN\n",
      "Hindustan | PROPN\n",
      "Unilever | PROPN\n",
      "and | CCONJ\n",
      "Bharti | PROPN\n",
      "Airtel | PROPN\n"
     ]
    }
   ],
   "source": [
    "lemma_txt=''\n",
    "for org in exercise2:\n",
    "    if org.pos_ != \"PUNCT\":\n",
    "        print(f\"{org} | {org.pos_}\")\n",
    "        lemma_txt += ''.join(org.text)+' '\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8093ae52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The Top 5 companies in USA are Tesla Walmart Amazon Microsoft asdfasdfas Google and the top 5 companies in \\n India are Infosys Reliance HDFC Bank Hindustan Unilever and Bharti Airtel '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89a34007",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = nlp(lemma_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee67f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA | PROPN\n",
      "Tesla | PROPN\n",
      "Walmart | PROPN\n",
      "Amazon | PROPN\n",
      "Microsoft | PROPN\n",
      "Google | PROPN\n",
      "India | PROPN\n",
      "Infosys | PROPN\n",
      "Reliance | PROPN\n",
      "HDFC | PROPN\n",
      "Bank | PROPN\n",
      "Hindustan | PROPN\n",
      "Unilever | PROPN\n",
      "Bharti | PROPN\n",
      "Airtel | PROPN\n"
     ]
    }
   ],
   "source": [
    "for token in text3:\n",
    "    if token.pos_ ==\"PROPN\":\n",
    "        print(f\"{token} | {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a8eab49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "com =[]\n",
    "for token in text3.ents:\n",
    "    if token.label_ == \"ORG\":\n",
    "        com.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94f7e6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tesla Walmart, Microsoft, Google, Infosys Reliance HDFC Bank, Bharti]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "91babb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 =''' The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "77053a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nlp(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1bc437b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla\n",
      "Walmart\n",
      "Amazon\n",
      "Microsoft\n",
      "Google\n",
      "Infosys\n",
      "Reliance\n",
      "HDFC Bank\n",
      "Hindustan Unilever\n",
      "Bharti\n"
     ]
    }
   ],
   "source": [
    "comp = []\n",
    "for models in model.ents:\n",
    "    if models.label_ == \"ORG\":\n",
    "        print(models)\n",
    "        comp.append(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0a9823e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tesla,\n",
       " Walmart,\n",
       " Amazon,\n",
       " Microsoft,\n",
       " Google,\n",
       " Infosys,\n",
       " Reliance,\n",
       " HDFC Bank,\n",
       " Hindustan Unilever,\n",
       " Bharti]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2bd7c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_details = {}\n",
    "for comp_list in comp:\n",
    "    if comp_list in comp_details:\n",
    "        comp_details[comp_list]+=1\n",
    "    else:\n",
    "        comp_details[comp_list]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "19d20788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla : 1\n",
      "Walmart : 1\n",
      "Amazon : 1\n",
      "Microsoft : 1\n",
      "Google : 1\n",
      "Infosys : 1\n",
      "Reliance : 1\n",
      "HDFC Bank : 1\n",
      "Hindustan Unilever : 1\n",
      "Bharti : 1\n"
     ]
    }
   ],
   "source": [
    "for list,count in comp_details.items():\n",
    "    print(f\"{list} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c0de2144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a76393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
